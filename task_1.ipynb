{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import csv\n",
    "from io import StringIO\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on home server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    connection = mysql.connector.connect(\n",
    "        host = '192.168.1.26',\n",
    "        user = 'aportra',\n",
    "        password = '@$Occer22',\n",
    "        database = 'wedge_assignment')\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "except mysql.connector.Error as err:\n",
    "    print(f'{err}')\n",
    "\n",
    "finally:\n",
    "    if connection.is_connected():\n",
    "        connection.close()\n",
    "        print('Connection Closed')\n",
    "    \n",
    "    if cursor:\n",
    "        print('Cursor created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Directory for Mac and Windows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    directory = '/Users/aaronportra/Documents/Advanced Data Analytics/wedge_project/WedgeZipOfZips_small/'\n",
    "    os.listdir(directory)\n",
    "except FileNotFoundError:\n",
    "    directory = 'C:\\\\Users\\\\aport\\\\OneDrive\\\\Documents\\\\School\\\\Applied Data Analytics\\\\wedge_project\\\\WedgeZipOfZips_small\\\\'\n",
    "    os.listdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data for BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sniff(file_path):\n",
    "    with open(file_path, 'r', errors = 'replace') as f:\n",
    "        sample = f.read(10000)\n",
    "\n",
    "        sniffer = csv.Sniffer()\n",
    "\n",
    "        f.seek(0)\n",
    "\n",
    "        delimiter = sniffer.sniff(sample).delimiter\n",
    "\n",
    "        f.seek(0)\n",
    "\n",
    "        reader = csv.reader(f, delimiter=delimiter)\n",
    "\n",
    "        first_row = next(reader)\n",
    "\n",
    "        has_header = sniffer.has_header(sample)\n",
    "\n",
    "\n",
    "        if len(first_row) < 50:\n",
    "            has_header = False\n",
    "        # if :\n",
    "\n",
    "        #     has_header = False\n",
    "\n",
    "    return delimiter, has_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sniff(directory + 'transArchive_201607_small.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sniff(directory + 'transArchive_201201_201203_inactive_small.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = {}\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith('.zip'):\n",
    "        with zipfile.ZipFile(directory + file,'r') as zip_file:\n",
    "            for info in zip_file.namelist():\n",
    "                h = sniff(directory + file)\n",
    "\n",
    "                file_content = zip_file.read(info).decode('utf-8')\n",
    "\n",
    "                csv_file = StringIO(file_content)\n",
    "\n",
    "                data = pd.read_csv(csv_file, delimiter = h[0])\n",
    "\n",
    "                data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "                if h[1]:\n",
    "\n",
    "                    columns = data.columns\n",
    "\n",
    "                if not h[1]:\n",
    "                     \n",
    "                     data.columns = columns\n",
    "                \n",
    "                transactions[file] = data\n",
    "\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in transactions['transArchive_201609_small.zip'].columns:\n",
    "    if col not in transactions['transArchive_201001_201003_small.zip'].columns:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transactions['transArchive_201001_201003_small.zip'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,trans in enumerate(transactions):\n",
    "   assert(len(transactions[trans].columns) == 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should iterate through each to establish columns that contain NaN. Then can focus on how to handle each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_cols = set()\n",
    "\n",
    "for trans in transactions:\n",
    "    for cols in transactions[trans]:\n",
    "        if pd.isna(transactions[trans][cols])[0] == True:\n",
    "            nan_cols.add(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trans in transactions:\n",
    "#     for cols in transactions[trans]:\n",
    "#         if cols in nan_cols:\n",
    "#             print(cols,' ', trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_nans(dic):\n",
    "\n",
    "    for trans in dic:\n",
    "        for cols in dic[trans]:\n",
    "            for value in dic[trans][cols].values:\n",
    "                print(value)\n",
    "\n",
    "\n",
    "            # for df in dic[trans][cols]:\n",
    "            #     if df:\n",
    "            #         print(df)\n",
    "            #     # if dic[trans][cols].values.any == 'nan':\n",
    "            #     #     print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve_nans(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(transactions['transArchive_201001_201003_small.zip']['datetime'].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
